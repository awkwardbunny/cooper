    The copycat binary was run multiple times with a bash script with increasing buffer size in powers of two. Each execution read and wrote 50MB files, and were timed with the 'time' command. See "man 1 command" for more details. The tests were run on a x86_64 architecture quad-core machine with clock speed of 800MHz per core. The machine is running Arch Linux with version 4.7.4-1 of the Linux Kernel.
    Initially, the throughput (measured in MB/s) seems to increase with the increase of the buffer size. It increases until the buffer size is 2^14 (16k) large. When the buffer size is greater than equal to 16KB, the throughput drops and stays around 270MB/s. (Tested only up to 256K; no extra conclusion about larger sizes can me made) This gives us the optimal performance (MB/s) when the buffer is 2^13 bytes (8k) large.
    This number probably comes from the block read size the kernel performs to get the data from the drive. When the buffer is smaller than the block, it is wasting the rest of the read block and therefore extra calls are necessary (unless caching is done). When the buffer size equals the read block size, it is optimal since no data read is being wasted and no extra calls have to be made to fill up the buffer.
    When the buffer size is larger than the block read size, multiple blocks have to be read from the drive, which reduces the throughput greatly since drive I/O operations are very slow and costly.
    From this, I guessed that the block size the kernel reads at once from the drive must have been 8KBs. And since every larger sizes I've tried are multiples of 8KB, it makes sense that the throughput stayed approximately the same.
    (But why 8K? Isn't the page size on 64-bits still 4K?)
